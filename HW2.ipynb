{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BA 820 Homework 2\n",
    "\n",
    "Group Name:\n",
    "Member Names:\n",
    "\n",
    "Reminder: you should not be sharing code across groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Clustering emails using Gaussian Mixture Models [50 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we will be analyzing emails taken from the [Enron Email Dataset](https://www.cs.cmu.edu/~enron/). We have already curated the data for you in the `data/enron` directory, with one file per email (so there is no need to redownload the dataset as it is already included in this homework zip archive). The dataset includes a combination of \"spam\" and \"ham\" emails. Spam emails are unsolicited messages intended to sell a product or scam users into providing personal information, while ham emails represent everything that is not spam.\n",
    "\n",
    "The language used in spam emails tends to be considerably different from typical business emails. Most spam filters leverage this difference in word frequencies to detect spam as it arrives and filter it out of email inboxes. We will use Gaussian Mixture Models (GMMs) to cluster the emails and attempt to identify groups of emails which are likely to be spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Construct a list of processed documents [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided below is a function that returns a document present in a file given a filename. The function performs some preprocessing to (1) remove punctuation, (2),(3) remove whitespace and (4) lowercase all words. Using the `make_word_list` function, construct a list of processed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import codecs\n",
    "\n",
    "def make_word_list(path):\n",
    "    \n",
    "    with codecs.open(path, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "        corpus_text = f.read()\n",
    "\n",
    "    for c in string.punctuation:\n",
    "        corpus_text = corpus_text.replace(c, \"\")  # -- (1)\n",
    "    \n",
    "    text = re.sub(r'\\S*\\d\\S*', '', corpus_text) # -- (2)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)         # -- (3)\n",
    "    \n",
    "    text = text.lower().split()           # -- (4)         \n",
    "    \n",
    "    li = []\n",
    "    for token in text:\n",
    "        li.append(token)\n",
    "\n",
    "    return \" \".join(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Construct a document matrix [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, construct a document matrix `X` as a matrix of [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) features for each document. TF-IDF features encode information about the word frequency within a document, weighted by the frequency of the same word for corpus as a whole.\n",
    "\n",
    "   - Use the `TfidfVectorizer` from [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to create the document matrix `X`.\n",
    "   - Set `min_df=50`; this drops words that don't occur in at least 50 documents.\n",
    "   - Set `stop_words=\"english\"` and `max_df=0.8` to filter out stop-words.\n",
    "   - Print the number of unique words in the vocabulary Hint: Look for the `vocabulary_` attribute in the `TfidfVectorizer` object after fitting the vectorizer to the data set. The vocabulary will also be needed in part (d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Perform Clustering [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform clustering on the document matrix `X` using GMMs. \n",
    "- Use the `GaussianMixture` module from [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) to cluster the documents.\n",
    "- Set the number of clusters (mixture components) to 3.\n",
    "- Print the number of documents assigned to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Characterize the clusters [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will attempt to characterize the clusters based on the frequency of spam-relevant words. \n",
    "- Select 3 \"spammy\" words from the vocabulary that you expect to occur frequently in spam emails and less frequently in normal business emails (e.g., 'click', 'www', 'offer'). Construct a list, `spam_indices`, storing the indices of the spammy words you selected in the vocabulary generated by `TfidfVectorizer`.\n",
    "- Select 3 \"hammy\" words from the vocabulary you think will occur frequently in business emails and infrequently in spam emails (e.g., 'meeting', 'attached', 'review'). Construct a list, `ham_indices`, storing the indices of the hammy words you selected in the vocabulary generated by `TfidfVectorizer`.\n",
    "- Use the function provided below, `plot_spam_words_by_cluster`, to plot the average TF-IDF features for your selected spammy words and hammy words for each cluster. The function takes the following parameters:\n",
    "    - `X` is a matrix of TF-IDF features (rows index documents, columns index terms).\n",
    "    - `predicted` is predicted cluster assignments returned by `GaussianMixture.predict()`\n",
    "    - `n_clusters` is the number of clusters. This should match the number of clusters from your GMM.\n",
    "    - `spam_indices` is a list of indices of spammy words in the vocabulary.\n",
    "    - `ham_indices` is a list of indices of hammy (typical business) words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spam_words_by_cluster(X, predicted, n_clusters, spam_indices, ham_indices):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Calculate mean TF-IDF features for each cluster\n",
    "    mean_tfidf_spam = []\n",
    "    mean_tfidf_ham = []\n",
    "    for spam in spam_indices:\n",
    "        mean_tfidf_spam.append([])\n",
    "    for ham in ham_indices:\n",
    "        mean_tfidf_ham.append([])\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        for j, spam_idx in enumerate(spam_indices):\n",
    "            mean_tfidf_spam[j].append(X[predicted==i, spam_idx].mean())\n",
    "        for j, ham_idx in enumerate(ham_indices):\n",
    "            mean_tfidf_ham[j].append(X[predicted==i, ham_idx].mean())\n",
    "    \n",
    "    # Generate plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ind = np.arange(n_clusters)    # the x locations for the groups\n",
    "    width = 0.10         # the width of the bars\n",
    "\n",
    "    # Plot spam bars\n",
    "    for i, val in enumerate(mean_tfidf_spam):\n",
    "        spam_bar = ax.bar(ind - (len(mean_tfidf_spam)-i-1)*width, val, width, color='r', bottom=0)\n",
    "\n",
    "    # Plot ham bars\n",
    "    for i, val in enumerate(mean_tfidf_ham):\n",
    "        ham_bar = ax.bar(ind + (len(mean_tfidf_ham)-i)*width, val, width,\n",
    "                color='y', bottom=0)\n",
    "\n",
    "    ax.set_title('Mean TF-IDF Features by Cluster for Spam-Relevant Words')\n",
    "    ax.set_xticks(ind + width / 2)\n",
    "    xticklabels = []\n",
    "    for i in range(n_clusters):\n",
    "        xticklabels.append('Cluster ' + str(i) + '\\n(n=' + str((predicted==i).sum()) + ')')\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "\n",
    "    ax.legend((spam_bar[0], ham_bar[0]), ('Spammy words', 'Hammy words'))\n",
    "    ax.autoscale_view()\n",
    "    ax.set_ylabel('Mean TF-IDF')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E: Interpret results [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot generated in part (d), do any of the clusters correspond to spam emails, based on the frequency of spammy and hammy words? Do any of the clusters appear to correspond to typical business emails (ham)? \n",
    "\n",
    "Re-run your analysis with a different choice for the number of clusters in your GMM (e.g., `n_clusters=5` or `n_clusters=2`). How does changing the number of clusters affect your results? Does one choice of `n_clusters` appear better than another for locating clusters of spam emails?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** WRITE YOUR ANSWER HERE IN TEXT ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Latent \"Purposes\" in Mobile Apps [50 points]\n",
    "\n",
    "\n",
    "Think of Android applications as \"documents\", where \"words\" are the \"permissions\" that each app requests. Each app has a set of latent \"purposes\" (topics); a \"purpose\" may be a specific task (such as taking photographs and uploading them to the internet), and an application may have a mixture of purposes of varying strength.\n",
    "\n",
    "Can we uncover these purposes from a dataset of Android applications and the permissions they request?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Load the data [0 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [dataset](http://www.mariofrank.net/andrApps/) contains ~180,000 applications from the Android app store, including each app's name, description and permissions requested. For more about Android app permissions, see [this report](http://www.pewinternet.org/2015/11/10/an-analysis-of-android-app-permissions/).\n",
    "\n",
    "   * Download the data and unzip it into the same folder as this notebook.\n",
    "   * Rename the CSV file to \"android.csv\".\n",
    "   * Run the cells below to load the data into the notebook.\n",
    "\n",
    "This may take up ~2GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"android.csv\", sep=\",\",  thousands=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Number of ratings\"] = df[\"Number of ratings\"].astype(int) # fix data type\n",
    "df = df.drop_duplicates(subset=[\"App\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Convert each app into a \"document\" [0 points]\n",
    "\n",
    "The code below does the following:\n",
    "\n",
    "   * Filter out all the columns except the permission columns.\n",
    "   * Convert this dataframe into a numpy matrix.\n",
    "\n",
    "There are thus 180295 apps (documents) and 173 permissions (words).\n",
    "\n",
    "   - `X` is the \"document\" or \"word-frequency\" matrix.\n",
    "   - `permission_columns` contains the words ordered by their column index in X.\n",
    "   - `app_names` contains the application names ordered by their row index in X.\n",
    "   - `app_ratings` contains the *number* of application ratings (not the ratings themselves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permission_columns = list(df.columns[10:])\n",
    "app_names = list(df[\"App\"])\n",
    "app_ratings = np.array(df[\"Number of ratings\"])\n",
    "df_perms = df[permission_columns]\n",
    "X = df_perms.values\n",
    "#del df # uncomment to free up RAM\n",
    "#del df_perms # uncomment to free up RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Downweight \"stopwords\" [2.5 points]\n",
    "\n",
    "There are some permissions requested by almost all applications (like \"INTERNET\"), they are similar to stop-words (\"the\", \"and\", etc.) in natural language. We can downweight them by TF-IDF normalization.\n",
    "\n",
    "   * Use the `TfidfTransformer` class in `scikit-learn` to transform X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Discover 10 \"topics\" [2.5 points]\n",
    "\n",
    "   * Use the `LatentDirichletAllocation` class in `scikit-learn` to fit 10 topics to the data.\n",
    "   * Use 10 max iterations, set `n_jobs=-1` to use all cores on your machine (if it helps).\n",
    "   * Feel free to understand and play around with other parameters.\n",
    "   * This may take around 3 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each app can be viewed as a distribution over these 10 topics, and each topic is a distribution over all the permissions in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E: List the top 10 \"words\" in each topic [15 points]\n",
    "\n",
    "Do any of the topics correspond to meaningful app \"purposes\"? List the ones you find meaningful below:\n",
    "\n",
    "   * Topic #i: \"your interpretation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** WRITE YOUR ANSWER HERE IN TEXT ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F: For each topic, list the top 3 apps with at least 100000 ratings having the highest probability assigned to that topic [15 points]\n",
    "\n",
    "   - This further helps us understand what the topics actually mean.\n",
    "   - However, the data is noisy and this exercise may result in listing a bunch of obscure apps.\n",
    "   - Hence, filtering out all apps with less than 100000 ratings may help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part G: Visualize the document-topic proportion matrix in 2-D using t-SNE [15 points]\n",
    "\n",
    "Does our reduced dimensional space contain any structure? Visualize the documents (in the topic-space) using t-SNE.\n",
    "\n",
    "   * Use `TSNE` from `scikit-learn`.\n",
    "   * Set the angle to 0.99 and init to 'PCA'. Pick any learning rate that gives you a visually pleasing result.\n",
    "   * You may `fit` TSNE to a sample of the data (>1000 documents) instead of the full data.\n",
    "   * Remember to call `fit_transform` to obtain your 2-D data.\n",
    "   * Plot the t-SNE dimensions as a scatterplot.\n",
    "   * Color each point based on its most probable topic. Make sure you include a legend (10 colors).\n",
    "   \n",
    "Clusters in the t-SNE plot indicate groups of apps that have similar topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
